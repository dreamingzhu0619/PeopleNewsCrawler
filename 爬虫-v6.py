import nest_asyncio, asyncio
nest_asyncio.apply()   # æ‰“è¡¥ä¸ï¼Œå…è®¸åµŒå¥—äº‹ä»¶å¾ªç¯
import os
import time
import json
import re
import requests
from datetime import datetime, timedelta
import pandas as pd
from bs4 import BeautifulSoup
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import chardet

# ========== æ–‡ä»¶è·¯å¾„ ==========
DESKTOP = os.path.expanduser("~/Desktop")
OUT_XLSX = os.path.join(DESKTOP, "people_news_final_deduped.xlsx")
TEMP_CSV = os.path.join(DESKTOP, "people_news_backup_final_deduped.csv")

# ========== æ—¶é—´èŒƒå›´ ==========
START_DATE = datetime(2024, 4, 1)
END_DATE = datetime(2024, 5, 1)
WINDOW = 1  # å¤©

# ========== æœç´¢å…³é”®è¯ ==========
SEARCH_KEY = "æ”¿ç­–"

# ========== API é…ç½® ==========
API_URL = "http://search.people.cn/search-platform/front/search"
HEADERS = {
    "Content-Type": "application/json;charset=UTF-8",
    "User-Agent": "Mozilla/5.0",
    # ã€é‡è¦ã€‘è¯·åŠ¡å¿…æ›´æ–°æ‚¨çš„Cookie
    "Cookie": "__jsluid_h=701a2860ad98e437adf837c0f2588434; sso_c=0; sfr=1"
}

BASE_PAYLOAD = {
    "key": SEARCH_KEY, "page": 1, "limit": 10, "hasTitle": True,
    "hasContent": True, "isFuzzy": True, "type": 1, "sortType": 2,
}

# ========== å…³é”®è¯ ==========
KEYWORD_SET = {
    "ç»æµ", "é‡‘è", "å•†ä¸š", "ä¸ç¡®å®š", "ä¸æ˜ç¡®", "æœªæ˜", "ä¸æ˜æœ—", "ä¸æ¸…æ™°", "æœªæ¸…æ™°",
    "éš¾æ–™", "éš¾ä»¥é¢„æ–™", "éš¾ä»¥é¢„æµ‹", "éš¾ä»¥é¢„è®¡", "éš¾ä»¥ä¼°è®¡", "æ— æ³•é¢„æ–™", "æ— æ³•é¢„æµ‹",
    "æ— æ³•é¢„è®¡", "æ— æ³•ä¼°è®¡", "ä¸å¯é¢„æ–™", "ä¸å¯é¢„æµ‹", "ä¸å¯é¢„è®¡", "ä¸å¯ä¼°è®¡", "æ³¢åŠ¨",
    "éœ‡è¡", "åŠ¨è¡", "ä¸ç¨³", "æœªçŸ¥", "æ”¿ç­–", "åˆ¶åº¦", "ä½“åˆ¶", "æˆ˜ç•¥", "æªæ–½", "è§„ç« ",
    "è§„ä¾‹", "æ¡ä¾‹", "æ”¿æ²»", "æ‰§æ”¿", "æ”¿åºœ", "å›½åŠ¡é™¢", "äººå¤§", "äººæ°‘ä»£è¡¨å¤§ä¼š",
    "ä¸­å¤®", "å›½å®¶é¢†å¯¼äºº", "æ€»ç†", "æ”¹é©", "æ•´æ”¹", "æ•´æ²»", "è§„ç®¡", "ç›‘ç®¡", "è´¢æ”¿",
    "ç¨", "äººæ°‘é“¶è¡Œ", "å¤®è¡Œ", "èµ¤å­—", "åˆ©ç‡"
}
# ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ä»¥æ•è·æ›´å¤šç›¸å…³æ–‡ç« 
KEYWORD_PATTERN = re.compile('|'.join(KEYWORD_SET))


# ========== æ ¸å¿ƒå‡½æ•° ==========

def get_uniqueness_key(row):
    """ã€æ–°å¢ã€‘æ ¹æ®æ ‡é¢˜å’Œæ—¥æœŸç”Ÿæˆæ–‡ç« çš„å”¯ä¸€â€œæŒ‡çº¹â€"""
    title = row.get('æ ‡é¢˜', '')
    # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—çš„å­—ç¬¦ï¼ŒæŠµæŠ—æ’ç‰ˆå·®å¼‚
    cleaned_title = re.sub(r'\W+', '', title)
    # å–æ ‡é¢˜å‰50ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹ï¼Œé¿å…é•¿æ ‡é¢˜æœ«å°¾çš„å¾®å°å·®å¼‚
    title_fingerprint = cleaned_title[:50]
    date_str = row.get('æ—¶é—´', '').split(' ')[0]  # ç¡®ä¿åªå–æ—¥æœŸ
    return f"{date_str}_{title_fingerprint}"


def text_hits(text):
    text = text.replace(" ", "")
    hits = set(KEYWORD_PATTERN.findall(text))
    return list(hits)


def clean(x):
    return re.sub(r"\s+", " ", str(x or "")).strip()


def parse_time(x):
    if not x: return None
    try:
        if isinstance(x, (int, float)):
            return datetime.fromtimestamp(int(x / 1000) if x > 1e11 else int(x))
        return datetime.strptime(x[:10], "%Y-%m-%d")
    except (ValueError, TypeError):
        return None


async def fetch_html_async(session, url, retries=3):
    headers = {"User-Agent": "Mozilla/5.0"}
    for attempt in range(retries):
        try:
            async with session.get(url, headers=headers, timeout=15) as response:
                if response.status == 200:
                    content = await response.read()
                    encoding = chardet.detect(content)['encoding'] or 'utf-8'
                    return content.decode(encoding, errors='ignore')
                elif response.status in [429, 500, 502, 503, 504]:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return ""
        except asyncio.TimeoutError:
            await asyncio.sleep(2 ** attempt)
        except Exception:
            return ""
    print(f"    âŒ å¤šæ¬¡é‡è¯•å¤±è´¥: {url}")
    return ""


def extract_body(html):
    if not html: return ""
    soup = BeautifulSoup(html, "lxml")
    selectors = ["#rwb_zw", ".rm_txt_con", ".rm_txt", ".article-content", ".article",
                 ".content", ".main-content", "#articleContent", ".text_content"]
    for sel in selectors:
        node = soup.select_one(sel)
        if node:
            for tag in node.select("script,style,.zdfy,.editor,.related-news,a,span"):
                tag.decompose()
            body = clean(node.get_text(" "))
            if len(body) > 50: return body
    paragraphs = soup.find_all('p')
    if paragraphs:
        body = clean(' '.join([p.get_text(" ") for p in paragraphs]))
        if len(body) > 50: return body
    return ""


async def process_news_batch(news_items):
    async with aiohttp.ClientSession() as session:
        tasks, valid_items = [], []
        for item in news_items:
            url = clean(item.get("url") or item.get("originUrl"))
            if not url or not url.startswith("http") or "video" in url: continue
            valid_items.append(item)
            tasks.append(fetch_html_async(session, url))

        if not tasks: return []
        print(f"  ğŸš€ å¹¶å‘è¯·æ±‚ {len(tasks)} ä¸ªURL")
        htmls = await asyncio.gather(*tasks)

        with ThreadPoolExecutor(max_workers=10) as executor:
            loop = asyncio.get_event_loop()
            extract_tasks = [loop.run_in_executor(executor, extract_body, html) for html in htmls]
            bodies = await asyncio.gather(*extract_tasks)

        results = []
        for item, body in zip(valid_items, bodies):
            title = clean(item.get("title") or "æ— æ ‡é¢˜")
            if not body or len(body) < 100:
                print(f"    ğŸ“„ æ­£æ–‡è¿‡çŸ­æˆ–ä¸ºç©ºï¼Œè·³è¿‡: {title}")
                continue
            hits = text_hits(body)
            if not hits:
                print(f"    ğŸ” æœªå‘½ä¸­å…³é”®è¯ï¼Œè·³è¿‡: {title}")
                continue

            pub = parse_time(item.get("displayTime") or item.get("publishTime"))
            results.append({
                "æ ‡é¢˜": title,
                "æ—¶é—´": pub.strftime("%Y-%m-%d") if pub else "",
                "URL": clean(item.get("url")),
                "æ­£æ–‡": body, "å­—æ•°": len(body), "å‘½ä¸­å…³é”®è¯": ",".join(hits)
            })
        return results


# ========== ä¸»é€»è¾‘ (å¸¦æ™ºèƒ½å»é‡) ==========
def main():
    # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨â€œå”¯ä¸€æŒ‡çº¹â€ä¸ºé”®çš„å­—å…¸ï¼Œå®ç°å†…å®¹å»é‡
    all_data = {}
    if os.path.exists(TEMP_CSV):
        try:
            df_backup = pd.read_csv(TEMP_CSV)
            for row in df_backup.to_dict("records"):
                key = get_uniqueness_key(row)
                all_data[key] = row
            print(f"âœ… ç¼“å­˜æ¢å¤ {len(all_data)} æ¡ä¸é‡å¤è®°å½•")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–ç¼“å­˜æ–‡ä»¶: {e}")

    session = requests.Session();
    session.headers.update(HEADERS)
    current, save_counter = END_DATE, 0
    save_interval = 20

    while current > START_DATE:
        prev = current - timedelta(days=WINDOW)
        print(f"\nğŸ“… æŠ“å–æ—¶é—´æ®µ: {prev.date()} â†’ {current.date()}")
        page = 1
        while True:
            payload = BASE_PAYLOAD.copy()
            payload.update(
                {"page": page, "startTime": int(prev.timestamp() * 1000), "endTime": int(current.timestamp() * 1000)})
            try:
                print(f"  ğŸ“¡ è¯·æ±‚APIç¬¬ {page} é¡µ")
                resp = session.post(API_URL, json=payload, timeout=12)
                resp.raise_for_status()
                recs = resp.json().get("data", {}).get("records") or []
            except Exception as e:
                print(f"âš ï¸ APIè¯·æ±‚å¤±è´¥: {e}ï¼Œä¼‘æ¯5ç§’");
                time.sleep(5);
                continue

            if not recs: print("  ğŸ“„ ç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šè®°å½•"); break
            print(f"  ğŸ“¥ ç¬¬ {page} é¡µè·å–åˆ° {len(recs)} æ¡è®°å½•")

            processed_rows = asyncio.run(process_news_batch(recs))
            newly_added_count = 0
            if processed_rows:
                for row in processed_rows:
                    key = get_uniqueness_key(row)
                    # ã€æ ¸å¿ƒå»é‡é€»è¾‘ã€‘
                    if key not in all_data:
                        all_data[key] = row
                        newly_added_count += 1
                        print(f"    â• æ–°å¢æ–‡ç« : {row['æ ‡é¢˜']}")
                    else:
                        # å¦‚æœæ˜¯é‡å¤æ–‡ç« ï¼Œä¿ç•™æ­£æ–‡æ›´é•¿çš„ç‰ˆæœ¬
                        if len(row['æ­£æ–‡']) > len(all_data[key]['æ­£æ–‡']):
                            print(f"    ğŸ”„ æ›´æ–°æ–‡ç«  (æ­£æ–‡æ›´é•¿): {row['æ ‡é¢˜']}")
                            all_data[key] = row
                        else:
                            print(f"    â– å‘ç°é‡å¤æ–‡ç« ï¼Œè·³è¿‡: {row['æ ‡é¢˜']}")

                save_counter += newly_added_count
                if save_counter >= save_interval:
                    print(f"\nğŸ’¾ è¾¾åˆ°ä¿å­˜é˜ˆå€¼ï¼Œæ­£åœ¨ä¿å­˜ {len(all_data)} æ¡æ•°æ®...")
                    try:
                        df = pd.DataFrame(list(all_data.values()))
                        df.to_excel(OUT_XLSX, index=False)
                        df.to_csv(TEMP_CSV, index=False, encoding="utf-8-sig")
                        save_counter = 0;
                        print("ğŸ’¾ ä¿å­˜å®Œæˆï¼\n")
                    except Exception as e:
                        print(f"âš ï¸ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            else:
                print("  ğŸ†• æœ¬æ‰¹æ¬¡æ— æœ‰æ•ˆæ–°è®°å½•")

            page += 1;
            time.sleep(0.5)
        current = prev

    print("\nğŸ‰ å®Œæˆï¼æ­£åœ¨ä¿å­˜æœ€ç»ˆæ•°æ®...")
    try:
        final_df = pd.DataFrame(list(all_data.values()))
        final_df.sort_values(by='æ—¶é—´', ascending=False, inplace=True)
        final_df.to_excel(OUT_XLSX, index=False)
        final_df.to_csv(TEMP_CSV, index=False, encoding="utf-8-sig")
        print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼æ–‡ä»¶ä¿å­˜ï¼š{OUT_XLSX}")
        print(f"ğŸ“Š ç»Ÿè®¡ï¼šå…±ä¿å­˜ {len(final_df)} æ¡ä¸é‡å¤çš„æ–°é—»")
    except Exception as e:
        print(f"âš ï¸ æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")


if __name__ == "__main__":
    main()